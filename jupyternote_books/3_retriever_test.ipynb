{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b39fb87f",
   "metadata": {},
   "source": [
    "# Retriever Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49fcdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import Dataset, concatenate_datasets, load_from_disk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import argparse\n",
    "import utils\n",
    "from pprint import pprint\n",
    "import konlpy.tag\n",
    "from transformers import AutoTokenizer\n",
    "from importlib import import_module\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6e0f4d",
   "metadata": {},
   "source": [
    "## 1. Loading parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2b9f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--num_clusters'], dest='num_clusters', nargs=None, const=None, default=64, type=<class 'int'>, choices=None, help='', metavar=64)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYDICT = {'key': 'value'}\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "parser.add_argument(\n",
    "    \"--retriever_path\",\n",
    "    default=\"\",\n",
    "    metavar=\"\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--config_retrieval\",\n",
    "    default=\"./config/retrieval_config.json\",\n",
    "    metavar=\"./config/retrieval_config.json\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vectorizer_type\",\n",
    "    default=\"TfidfVectorizer\",\n",
    "    metavar=\"TfidfVectorizer\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument('--vectorizer_parameters',\n",
    "                    type=json.loads, default=MYDICT)\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_type\",\n",
    "    default=\"AutoTokenizer\",\n",
    "    metavar=\"AutoTokenizer\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_name\", default = \"./data/train_dataset\",\n",
    "    metavar=\"./data/train_dataset\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default =\"bert-base-multilingual-cased\",\n",
    "    metavar=\"bert-base-multilingual-cased\",\n",
    "    type=str,\n",
    "    help=\"\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--top_k\",\n",
    "    default =10,\n",
    "    metavar=10,\n",
    "    type=int,\n",
    "    help=\"\",\n",
    ")\n",
    "parser.add_argument(\"--data_path\",default = \"./data\",\n",
    "                    metavar=\"./data\", type=str, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--context_path\",\n",
    "    default = \"wikipedia_documents.json\",\n",
    "    metavar=\"wikipedia_documents.json\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_path\",\n",
    "    default=\"./retriever_result\",\n",
    "    metavar=\"./retriever_result\", type=str, help=\"\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--use_faiss\", default=False, metavar=False, type=bool, help=\"\")\n",
    "parser.add_argument(\"--num_clusters\", default=64, metavar=64, type=int, help=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6b257c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "config = utils.read_json(args.config_retrieval)\n",
    "parser.set_defaults(**config)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be942970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'config_retrieval': './config/retrieval_config.json',\n",
      " 'context_path': 'wikipedia_documents.json',\n",
      " 'data_path': '../data',\n",
      " 'dataset_name': '../data/train_dataset',\n",
      " 'model_name_or_path': 'bert-base-multilingual-cased',\n",
      " 'num_clusters': 64,\n",
      " 'output_path': './retriever_result',\n",
      " 'retriever_path': '',\n",
      " 'tokenizer_type': 'AutoTokenizer',\n",
      " 'top_k': 10,\n",
      " 'use_faiss': False,\n",
      " 'vectorizer_parameters': {'ngram_range': [1, 2]},\n",
      " 'vectorizer_type': 'TfidfVectorizer'}\n"
     ]
    }
   ],
   "source": [
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a32ef1",
   "metadata": {},
   "source": [
    "## 2. loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b56a3b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/train_dataset/train/cache-9681ee696ea809ac.arrow\n",
      "Loading cached processed dataset at ../data/train_dataset/validation/cache-39f91efb8d01b7c9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************** query dataset ****************************************\n",
      "Dataset({\n",
      "    features: ['__index_level_0__', 'answers', 'context', 'document_id', 'id', 'question', 'title'],\n",
      "    num_rows: 4192\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "org_dataset = load_from_disk(args.dataset_name)\n",
    "full_ds = concatenate_datasets(\n",
    "    [\n",
    "        org_dataset[\"train\"].flatten_indices(),\n",
    "        org_dataset[\"validation\"].flatten_indices(),\n",
    "    ]\n",
    ")  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da1044ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4192\n",
      "4192\n",
      "4192\n"
     ]
    }
   ],
   "source": [
    "print(len(full_ds[\"question\"]))\n",
    "print(len(full_ds[\"context\"]))\n",
    "print(len(full_ds[\"answers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41eeb218",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question : 대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\n",
      "contenxt : 미국 상의원 또는 미국 상원(United States Senate)은 양원제인 미국 의회의 상원이다.\\n\\n미국 부통령이 상원의장이 된다. 각 주당 2명의 상원의원이 선출되어 100명의 상원의원으로 구성되어 있다. 임기는 6년이며, 2년마다 50개주 중 1/3씩 상원의원을 새로 선출하여 연방에 보낸다.\\n\\n미국 상원은 미국 하원과는 다르게 미국 대통령을 수반으로 하는 미국 연방 행정부에 각종 동의를 하는 기관이다. 하원이 세금과 경제에 대한 권한, 대통령을 포함한 대다수의 공무원을 파면할 권한을 갖고 있는 국민을 대표하는 기관인 반면 상원은 미국의 주를 대표한다. 즉 캘리포니아주, 일리노이주 같이 주 정부와 주 의회를 대표하는 기관이다. 그로 인하여 군대의 파병, 관료의 임명에 대한 동의, 외국 조약에 대한 승인 등 신속을 요하는 권한은 모두 상원에게만 있다. 그리고 하원에 대한 견제 역할(하원의 법안을 거부할 권한 등)을 담당한다. 2년의 임기로 인하여 급진적일 수밖에 없는 하원은 지나치게 급진적인 법안을 만들기 쉽다. 대표적인 예로 건강보험 개혁 당시 하원이 미국 연방 행정부에게 퍼블릭 옵션(공공건강보험기관)의 조항이 있는 반면 상원의 경우 하원안이 지나치게 세금이 많이 든다는 이유로 퍼블릭 옵션 조항을 제외하고 비영리건강보험기관이나 보험회사가 담당하도록 한 것이다. 이 경우처럼 상원은 하원이나 내각책임제가 빠지기 쉬운 국가들의 국회처럼 걸핏하면 발생하는 의회의 비정상적인 사태를 방지하는 기관이다. 상원은 급박한 처리사항의 경우가 아니면 법안을 먼저 내는 경우가 드물고 하원이 만든 법안을 수정하여 다시 하원에 되돌려보낸다. 이러한 방식으로 단원제가 빠지기 쉬운 함정을 미리 방지하는 것이다.날짜=2017-02-05\n",
      "answers : {'answer_start': [235], 'text': ['하원']}\n",
      "question : 현대적 인사조직관리의 시발점이 된 책은?\n",
      "contenxt : '근대적 경영학' 또는 '고전적 경영학'에서 현대적 경영학으로 전환되는 시기는 1950년대이다. 2차 세계대전을 마치고, 6.25전쟁의 시기로 유럽은 전후 재건에 집중하고, 유럽 제국주의의 식민지가 독립하여 아프리카, 아시아, 아메리카 대륙에서 신생국가가 형성되는 시기였고, 미국은 전쟁 이후 경제적 변화에 기업이 적응을 해야 하던 시기였다. 특히 1954년 피터 드러커의 저서 《경영의 실제》는 현대적 경영의 기준을 제시하여서, 기존 근대적 인사조직관리를 넘어선 현대적 인사조직관리의 전환점이 된다. 드러커는 경영자의 역할을 강조하며 경영이 현시대 최고의 예술이자 과학이라고 주장하였고 , 이 주장은 21세기 인사조직관리의 역할을 자리매김했다.\\n\\n현대적 인사조직관리와 근대 인사조직관리의 가장 큰 차이는 통합이다. 19세기의 영향을 받던 근대적 경영학(고전적 경영)의 흐름은 기능을 강조하였지만, 1950년대 이후의 현대 경영학은 통합을 강조하였다. 기능이 분화된 '기계적인 기업조직' 이해에서 다양한 기능을 인사조직관리의 목적, 경영의 목적을 위해서 다양한 분야를 통합하여 '유기적 기업 조직' 이해로 전환되었다. 이 통합적 접근방식은 과정, 시스템, 상황을 중심으로 하는 인사조직관리 방식을 형성했다.\n",
      "answers : {'answer_start': [212], 'text': ['《경영의 실제》']}\n",
      "question : 강희제가 1717년에 쓴 글은 누구를 위해 쓰여졌는가?\n",
      "contenxt : 강희제는 강화된 황권으로 거의 황제 중심의 독단적으로 나라를 이끌어 갔기에 자칫 전제 독재의 가능성이 보일 수도 있었으나, 스스로 황권을 조절하고 정치의 일부는 재상들이나 대신들과 의논하였으며 당시 궁핍하게 살고 있는 한족들의 사정을 잘 알고 있던 한족 대신들의 의견을 수용하여 정책을 실행하고 선정을 베풀었다. 프랑스의 예수회 선교사 부베는 루이 14세에게 다음과 같이 보고하였다. \\n강희제는 세상에서 가장 부유한 군주입니다. 그럼에도 황제인 그의 생활용품들은 사치스러움과 화려함과는 거리가 멀다 못해 소박하기 그지없습니다. 역대 제왕들 가운데 전례없는 일입니다.\\n강희제 스스로도 자신이 직접 쓴 《근검록》에서 다음과 같이 쓰고 있다\\n모든 비용은 백성들의 피땀으로 얻어진 것이니 주인된 황제로서 절제하고 절제함은 당연한 것이 아닌가\\n\\n이런 강희제의 인자한 정치는 한족이 만주족의 청나라를 지지하게 만드는 데에 크게 일조하였다. 1717년(강희 56년) 강희제는 〈고별상유〉(告別上諭), 즉 마지막으로 백성들에게 바치는 글을 남겼는데 강희제는 “한 가지 일에 부지런하지 않으면 온 천하에 근심을 끼치고, 한 순간에 부지런하지 않으면 천추만대에 우환거리를 남긴다.”라고 역설하였다. 또한 “제왕이 천하를 다스림에 능력이 있는 자를 가까이 두고, 백성들의 세금을 낮추어 주어야 하며, 백성들의 마음을 하나로 묶고, 위태로움이 생기기 전에 나라를 보호하며, 혼란이 있기 전에 이를 먼저 파악하여 잘 다스리고, 관대하고 엄격함의 조화를 이루어 나라를 위한 계책을 도모해야 한다.”라고 후대의 황제에게도 이를 훈계하였다. 강희제는 황제로서 자식과 같은 백성들에게 이런 당부의 말을 남겨 황제로서의 도리를 다하려 하였다.\n",
      "answers : {'answer_start': [510], 'text': ['백성']}\n",
      "question : 11~12세기에 제작된 본존불은 보통 어떤 나라의 특징이 전파되었나요?\n",
      "contenxt : 불상을 모시기 위해 나무나 돌, 쇠 등을 깎아 일반적인 건축물보다 작은 규모로 만든 것을 불감(佛龕)이라고 한다. 불감은 그 안에 모신 불상의 양식뿐만 아니라, 당시의 건축 양식을 함께 살필 수 있는 중요한 자료가 된다. \\n\\n이 작품은 높이 18cm의 작은 불감으로, 청동으로 불감과 불상을 만들고 그 위에 금칠을 하였다. 불감 내부를 살펴보면 난간을 두른 사각형의 기단 위에 본존불과 양 옆에 보살상이 있으며, 그 위에 기둥과 지붕으로 된 뚜껑이 덮혀 있다. 법당 모양의 뚜껑에는 앞면과 양쪽에 커다란 창문이 있어서 안에 모셔진 불상을 잘 볼 수 있도록 하였다.\\n\\n본존불은 얼굴이 추상적이고, 양 어깨를 감싸고 있는 옷은 주름을 간략한 선으로 표현했다. 몸 뒤편에 있는 광배(光背)는 머리광배와 몸광배로 나누어져 있으며, 불꽃무늬로 가장자리를 장식하고 있다. 본존불 양 옆의 보살상도 구슬로 장식된 관(冠)을 쓰고 있다는 점을 제외하면 형식이나 표현 수법이 본존불과 유사하다.\\n\\n불감은 지금도 금색이 찬란하고 지붕에 녹청색이 남아 있는 등 전체적인 보존 상태가 양호하다. 본존불의 긴 허리, 불규칙하게 나타나는 옷주름, 그리고 보살이 쓰고 있는 구슬로 장식한 관(冠) 등 여러 양식으로 보아 만든 시기는 중국 북방 계통의 영향을 받은 11∼12세기 경으로 추정된다. 이 작품은 고려시대 또는 그 이전의 목조건축 양식과 조각수법을 보여주는 귀중한 예라는 점에서 가치가 크다고 할 수 있다.\n",
      "answers : {'answer_start': [625], 'text': ['중국']}\n",
      "question : 명문이 적힌 유물을 구성하는 그릇의 총 개수는?\n",
      "contenxt : 동아대학교박물관에서 소장하고 있는 계사명 사리구는 총 4개의 용기로 구성된 조선후기의 유물로, 경상남도 울주군 웅촌면 대복리에서 출토되었다고 전한다. 외함(外函)은 청화명문이 있는 백자이며, 그 안쪽에 납작한 금속제 원형 합 2점과 금속제 원통형 합 등 3점의 그릇이 봉안되어 있다.\\n\\n바깥쪽의 외함인 백자 합 동체 중앙부 표면에 청화안료로 쓴 “癸巳二月日 施主承表 兩主”라는 명문이 세로로 세 줄에 걸쳐서 쓰여 있어 조선 후기인 계사년에 시주자인 승표 부부가 발원하여 만든 것임을 알 수 있다.\\n\\n동아대학교박물관의 계사명 사리구는 정확한 제작연대는 알 수 없지만 명문 등을 통해 적어도 17세기 이후에 제작된 것으로 추정되는 작품으로, 명문이 있는 조선 후기 경상도 지역 출토 사리장엄구라는 점에서 중요한 가치를 지닌 작품으로 판단된다.\\n\\n조선 후기 사리장엄구는 아직까지 조사와 연구가 거의 이루어지지 않았으나, 이처럼 세트를 갖추어 출토된 유물은 비교적 드문 편임을 고려할 때, 이 계사명 사리장엄구는 제작연대와 발원자의 이름이 밝혀져 있으며, 지금까지 출토된 예가 드문 비교적 완전한 세트를 가진 유물이라는 점에서 조선 후기 사리장엄구 연구에 자료적 가치를 지닌 유물이다.\n",
      "answers : {'answer_start': [30], 'text': ['4개']}\n",
      "question : 카드모스의 부하들이 간 곳에는 무엇이 있었는가?\n",
      "contenxt : 델포이의 신탁에 따라 암소를 따라간 카드모스는 테베 땅에 이르렀다. 카드모스는 암소를 잡아서 신들에게 공양하려고 부하들에게 근처의 샘으로 물심부름을 보냈다. 샘은 드래곤이 지키고 있었고, 드래곤은 카드모스의 부하 여럿을 죽인 뒤 카드모스의 칼에 죽었다.\\n\\n《비블리오테카》에 따르면 이 드래곤은 아레스의 신수였다고 한다. 아테나는 드래곤의 이빨 중 절반을 카드모스에게 주고 그것을 땅에 심으라고 했다. 카드모스가 그렇게 하자 고랑마다 사나운 무장한 사내들이 튀어나왔다. 그들에게 겁을 먹은 카드모스는 그들 사이에 돌을 집어던졌고, 돌을 누가 던졌냐고 시비가 붙은 용아병들은 서로 싸우다가 다섯 명만 남기고 나머지는 모두 죽었다. 살아남은 용아병 다섯은 에키온, 우다에오스, 크토노니오스, 퓌헤레노르, 펠로루스이며, 이 다섯은 카드모스를 도와 테베라는 도시를 건립했다. 카드모스는 드래곤을 죽인 대가로 8년동안 아레스의 노예로 살았고, 그 기간이 끝나자 아레스와 아프로디테의 딸인 하르모니아를 아내로 맞았다. \\n\\n한편, 미틸레네의 헬라니코스의 판본에 따르면 애초부터 용아병은 다섯 명이 튀어나왔으며, 아레스가 카드모스를 죽이려고 하는 것을 제우스가 나서서 살려 주었다. 용아병들 중 에키온은 뒤에 카드모스의 딸 아가베와 결혼했고, 둘 사이에 태어난 아들 펜테우스가 카드모스의 뒤를 이어 테베의 왕이 되었다.\n",
      "answers : {'answer_start': [91], 'text': ['드래곤']}\n"
     ]
    }
   ],
   "source": [
    "for index, (q, c, a) in enumerate(zip(full_ds[\"question\"], full_ds[\"context\"], full_ds[\"answers\"])):\n",
    "    print(f'question : {q}')\n",
    "    print(f'contenxt : {c}')\n",
    "    print(f'answers : {a}')\n",
    "    if index == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb64219",
   "metadata": {},
   "source": [
    "## 3. Loading Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e662e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoTokenizer\n"
     ]
    }
   ],
   "source": [
    "if hasattr(import_module(\"transformers\"), args.tokenizer_type):\n",
    "    tokenizer_type = getattr(import_module(\"transformers\"), args.tokenizer_type)\n",
    "    tokenizer = tokenizer_type.from_pretrained(args.model_name_or_path, use_fast=False, )\n",
    "    print(f'{args.tokenizer_type}')\n",
    "elif hasattr(import_module(\"konlpy.tag\"), args.tokenizer_type):\n",
    "    tokenizer = getattr(import_module(\"konlpy.tag\"), args.tokenizer_type)()\n",
    "    print(f'{args.tokenizer_type}')\n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type - {args.tokenizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7854acd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizer(name_or_path='bert-base-multilingual-cased', vocab_size=119547, model_max_len=512, is_fast=False, padding_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a101eaa",
   "metadata": {},
   "source": [
    "## 4. Setting Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07c0fb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_path directory: ./retriever_result/TfidfVectorizer_bert-base-multilingual-cased_wikipedia_documents.json_5/\n"
     ]
    }
   ],
   "source": [
    "if args.tokenizer_type == \"AutoTokenizer\":\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.model_name_or_path}_{args.context_path}'\n",
    "else:\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.tokenizer_type}_{args.context_path}'\n",
    "output_path = utils.increment_directory(output_path)\n",
    "print(f'output_path directory: {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de287c2",
   "metadata": {},
   "source": [
    "## 5. Initializing SparseRetrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "439ea6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{name}] done in {time.time() - t0:.3f} s\")\n",
    "\n",
    "class SparseRetrieval:\n",
    "    def __init__(\n",
    "        self,\n",
    "        retrieval_path,\n",
    "        retrieval_type,\n",
    "        retrieval_parameters,\n",
    "        tokenize_fn,\n",
    "        output_path,\n",
    "        data_path: Optional[str] = \"../data/\",\n",
    "        context_path: Optional[str] = \"wikipedia_documents.json\",\n",
    "        num_clusters = 64\n",
    "    ) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            tokenize_fn:\n",
    "                기본 text를 tokenize해주는 함수입니다.\n",
    "                아래와 같은 함수들을 사용할 수 있습니다.\n",
    "                - lambda x: x.split(' ')\n",
    "                - Huggingface Tokenizer\n",
    "                - konlpy.tag의 Mecab\n",
    "\n",
    "            data_path:\n",
    "                데이터가 보관되어 있는 경로입니다.\n",
    "\n",
    "            context_path:\n",
    "                Passage들이 묶여있는 파일명입니다.\n",
    "\n",
    "            data_path/context_path가 존재해야합니다.\n",
    "\n",
    "        Summary:\n",
    "            Passage 파일을 불러오고 TfidfVectorizer를 선언하는 기능을 합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        self.data_path = data_path\n",
    "        with open(os.path.join(data_path, context_path), \"r\", encoding=\"utf-8\") as f:\n",
    "            wiki = json.load(f)\n",
    "\n",
    "        # 순서대로 중복 제거\n",
    "        # ('key1', 'key2', 'key3', 'key1', 'key4', 'key2') -> ['key1', 'key2', 'key3', 'key4']\n",
    "        self.contexts = list(\n",
    "            dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    "        )  # set 은 매번 순서가 바뀌므로\n",
    "\n",
    "        print(f\"Lengths of unique contexts : {len(self.contexts)}\")\n",
    "        self.ids = list(range(len(self.contexts)))\n",
    "\n",
    "        self.p_embedding = None  # get_sparse_embedding()로 생성합니다\n",
    "        self.indexer = None  # build_faiss()로 생성합니다.\n",
    "        self.num_clusters = num_clusters\n",
    "\n",
    "        self.get_sparse_embedding(retrieval_path, retrieval_type, tokenize_fn, retrieval_parameters, output_path)\n",
    "\n",
    "    def get_sparse_embedding(self, retriever_path, vectorizer_type, tokenize_fn, vectorizer_parameters, output_path) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            retriever_path 존재하면 해당 sparse retrieval loading\n",
    "            retriever_path 존재하지 않으면,\n",
    "                1) self.vectorizer 호출\n",
    "                2) Passage Embedding을 만들고\n",
    "                3) TFIDF와 Embedding을 pickle로 저장합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        # Pickle을 저장합니다.\n",
    "        pickle_name = f\"sparse_embedding.bin\"\n",
    "        vectorizer_name = f\"vectorizer.bin\"\n",
    "\n",
    "        if retriever_path:\n",
    "            print(f'Initializing sparse retriever on {retriever_path}')\n",
    "            emb_path = os.path.join(retriever_path, pickle_name)\n",
    "            vectorizer_path = os.path.join(self.data_path, vectorizer_name)\n",
    "\n",
    "            if os.path.isfile(emb_path) and os.path.isfile(vectorizer_path):\n",
    "                with open(emb_path, \"rb\") as file:\n",
    "                    self.p_embedding = pickle.load(file)\n",
    "                with open(vectorizer_path, \"rb\") as file:\n",
    "                    self.vectorizer = pickle.load(file)\n",
    "                print(f\"Passage embedding & Sparse Vectorizer Loaded from {retriever_path}\")\n",
    "        else:\n",
    "            print(f'Initializing new sparse retriever')\n",
    "            emb_path = os.path.join(output_path, pickle_name)\n",
    "            vectorizer_path = os.path.join(output_path, vectorizer_name)\n",
    "\n",
    "            # Transform by vectorizer\n",
    "            if hasattr(import_module(\"sklearn.feature_extraction.text\"), vectorizer_type):\n",
    "                vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), vectorizer_type)\n",
    "                self.vectorizer = vectorizer_type(tokenizer=tokenize_fn, **vectorizer_parameters)\n",
    "                print(f'{self.vectorizer}')\n",
    "\n",
    "            elif hasattr(import_module(\"retriever\"), vectorizer_type):\n",
    "                vectorizer_type = getattr(import_module(\"retriever\"), vectorizer_type)\n",
    "                self.vectorizer = vectorizer_type(tokenize_fn, vectorizer_parameters)\n",
    "                print(f'{self.vectorizer}')\n",
    "            else:\n",
    "                raise Exception(f\"Use correct tokenizer type : Current tokenizer : {vectorizer_type}\")\n",
    "\n",
    "            print(\"Build passage embedding\")\n",
    "            self.p_embedding = self.vectorizer.fit_transform(self.contexts)\n",
    "            print(self.p_embedding.shape)\n",
    "            with open(emb_path, \"wb\") as file:\n",
    "                pickle.dump(self.p_embedding, file)\n",
    "            with open(vectorizer_path, \"wb\") as file:\n",
    "                pickle.dump(self.vectorizer, file)\n",
    "            print(f\"Saving Passage embedding & Sparse Vectorizer to {output_path}\")\n",
    "\n",
    "\n",
    "    def build_faiss(self) -> NoReturn:\n",
    "\n",
    "        \"\"\"\n",
    "        Summary:\n",
    "            속성으로 저장되어 있는 Passage Embedding을\n",
    "            Faiss indexer에 fitting 시켜놓습니다.\n",
    "            이렇게 저장된 indexer는 `get_relevant_doc`에서 유사도를 계산하는데 사용됩니다.\n",
    "\n",
    "        Note:\n",
    "            Faiss는 Build하는데 시간이 오래 걸리기 때문에,\n",
    "            매번 새롭게 build하는 것은 비효율적입니다.\n",
    "            그렇기 때문에 build된 index 파일을 저정하고 다음에 사용할 때 불러옵니다.\n",
    "            다만 이 index 파일은 용량이 1.4Gb+ 이기 때문에 여러 num_clusters로 시험해보고\n",
    "            제일 적절한 것을 제외하고 모두 삭제하는 것을 권장합니다.\n",
    "        \"\"\"\n",
    "        num_clusters = self.num_clusters\n",
    "        indexer_name = f\"faiss_clusters{num_clusters}.index\"\n",
    "        indexer_path = os.path.join(self.data_path, indexer_name)\n",
    "        if os.path.isfile(indexer_path):\n",
    "            print(\"Load Saved Faiss Indexer.\")\n",
    "            self.indexer = faiss.read_index(indexer_path)\n",
    "\n",
    "        else:\n",
    "            p_emb = self.p_embedding.astype(np.float32).toarray()\n",
    "            emb_dim = p_emb.shape[-1]\n",
    "\n",
    "            num_clusters = num_clusters\n",
    "            quantizer = faiss.IndexFlatL2(emb_dim)\n",
    "\n",
    "            self.indexer = faiss.IndexIVFScalarQuantizer(\n",
    "                quantizer, quantizer.d, num_clusters, faiss.METRIC_L2\n",
    "            )\n",
    "            self.indexer.train(p_emb)\n",
    "            self.indexer.add(p_emb)\n",
    "            faiss.write_index(self.indexer, indexer_path)\n",
    "            print(\"Faiss Indexer Saved.\")\n",
    "\n",
    "    def retrieve(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.p_embedding is not None, \"get_sparse_embedding() 메소드를 먼저 수행해줘야합니다.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc(query_or_dataset, k=topk)\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(f\"Top-{i+1} passage with score {doc_scores[i]:4f}\")\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            total = []\n",
    "            with timer(\"query exhaustive search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk(\n",
    "                    query_or_dataset[\"question\"], k=topk\n",
    "                ) \n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context_id\": doc_indices[idx],\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            cqas = pd.DataFrame(total)\n",
    "            return cqas\n",
    "\n",
    "    def get_relevant_doc(self, query: str, k: Optional[int] = 1) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        with timer(\"transform\"):\n",
    "            query_vec = self.vectorizer.transform([query])\n",
    "        assert (\n",
    "            np.sum(query_vec) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        with timer(\"query ex search\"):\n",
    "            result = query_vec * self.p_embedding.T\n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = result.toarray()\n",
    "\n",
    "        sorted_result = np.argsort(result.squeeze())[::-1]\n",
    "        doc_score = result.squeeze()[sorted_result].tolist()[:k]\n",
    "        doc_indices = sorted_result.tolist()[:k]\n",
    "        return doc_score, doc_indices\n",
    "\n",
    "    def get_relevant_doc_bulk(\n",
    "        self, queries: List, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries (List):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        query_vec = self.vectorizer.transform(queries)\n",
    "        assert (\n",
    "            np.sum(query_vec) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        result = query_vec * self.p_embedding.T\n",
    "        if not isinstance(result, np.ndarray):\n",
    "            result = result.toarray()\n",
    "        doc_scores = []\n",
    "        doc_indices = []\n",
    "        for i in range(result.shape[0]):\n",
    "            sorted_result = np.argsort(result[i, :])[::-1]\n",
    "            doc_scores.append(result[i, :][sorted_result].tolist()[:k])\n",
    "            doc_indices.append(sorted_result.tolist()[:k])\n",
    "        return doc_scores, doc_indices\n",
    "\n",
    "    def retrieve_faiss(\n",
    "        self, query_or_dataset: Union[str, Dataset], topk: Optional[int] = 1\n",
    "    ) -> Union[Tuple[List, List], pd.DataFrame]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query_or_dataset (Union[str, Dataset]):\n",
    "                str이나 Dataset으로 이루어진 Query를 받습니다.\n",
    "                str 형태인 하나의 query만 받으면 `get_relevant_doc`을 통해 유사도를 구합니다.\n",
    "                Dataset 형태는 query를 포함한 HF.Dataset을 받습니다.\n",
    "                이 경우 `get_relevant_doc_bulk`를 통해 유사도를 구합니다.\n",
    "            topk (Optional[int], optional): Defaults to 1.\n",
    "                상위 몇 개의 passage를 사용할 것인지 지정합니다.\n",
    "\n",
    "        Returns:\n",
    "            1개의 Query를 받는 경우  -> Tuple(List, List)\n",
    "            다수의 Query를 받는 경우 -> pd.DataFrame: [description]\n",
    "\n",
    "        Note:\n",
    "            다수의 Query를 받는 경우,\n",
    "                Ground Truth가 있는 Query (train/valid) -> 기존 Ground Truth Passage를 같이 반환합니다.\n",
    "                Ground Truth가 없는 Query (test) -> Retrieval한 Passage만 반환합니다.\n",
    "            retrieve와 같은 기능을 하지만 faiss.indexer를 사용합니다.\n",
    "        \"\"\"\n",
    "\n",
    "        assert self.indexer is not None, \"build_faiss()를 먼저 수행해주세요.\"\n",
    "\n",
    "        if isinstance(query_or_dataset, str):\n",
    "            doc_scores, doc_indices = self.get_relevant_doc_faiss(\n",
    "                query_or_dataset, k=topk\n",
    "            )\n",
    "            print(\"[Search query]\\n\", query_or_dataset, \"\\n\")\n",
    "\n",
    "            for i in range(topk):\n",
    "                print(\"Top-%d passage with score %.4f\" % (i + 1, doc_scores[i]))\n",
    "                print(self.contexts[doc_indices[i]])\n",
    "\n",
    "            return (doc_scores, [self.contexts[doc_indices[i]] for i in range(topk)])\n",
    "\n",
    "        elif isinstance(query_or_dataset, Dataset):\n",
    "\n",
    "            # Retrieve한 Passage를 pd.DataFrame으로 반환합니다.\n",
    "            queries = query_or_dataset[\"question\"]\n",
    "            total = []\n",
    "\n",
    "            with timer(\"query faiss search\"):\n",
    "                doc_scores, doc_indices = self.get_relevant_doc_bulk_faiss(\n",
    "                    queries, k=topk\n",
    "                )\n",
    "            for idx, example in enumerate(\n",
    "                tqdm(query_or_dataset, desc=\"Sparse retrieval: \")\n",
    "            ):\n",
    "                tmp = {\n",
    "                    # Query와 해당 id를 반환합니다.\n",
    "                    \"question\": example[\"question\"],\n",
    "                    \"id\": example[\"id\"],\n",
    "                    # Retrieve한 Passage의 id, context를 반환합니다.\n",
    "                    \"context_id\": doc_indices[idx],\n",
    "                    \"context\": \" \".join(\n",
    "                        [self.contexts[pid] for pid in doc_indices[idx]]\n",
    "                    ),\n",
    "                }\n",
    "                if \"context\" in example.keys() and \"answers\" in example.keys():\n",
    "                    # validation 데이터를 사용하면 ground_truth context와 answer도 반환합니다.\n",
    "                    tmp[\"original_context\"] = example[\"context\"]\n",
    "                    tmp[\"answers\"] = example[\"answers\"]\n",
    "                total.append(tmp)\n",
    "\n",
    "            return pd.DataFrame(total)\n",
    "\n",
    "    def get_relevant_doc_faiss(\n",
    "        self, query: str, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            query (str):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        query_vec = self.vectorizer.transform([query])\n",
    "        assert (\n",
    "            np.sum(query_vec) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        q_emb = query_vec.toarray().astype(np.float32)\n",
    "        with timer(\"query faiss search\"):\n",
    "            D, I = self.indexer.search(q_emb, k)\n",
    "\n",
    "        return D.tolist()[0], I.tolist()[0]\n",
    "\n",
    "    def get_relevant_doc_bulk_faiss(\n",
    "        self, queries: List, k: Optional[int] = 1\n",
    "    ) -> Tuple[List, List]:\n",
    "\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            queries (List):\n",
    "                하나의 Query를 받습니다.\n",
    "            k (Optional[int]): 1\n",
    "                상위 몇 개의 Passage를 반환할지 정합니다.\n",
    "        Note:\n",
    "            vocab 에 없는 이상한 단어로 query 하는 경우 assertion 발생 (예) 뙣뙇?\n",
    "        \"\"\"\n",
    "\n",
    "        query_vecs = self.vectorizer.transform(queries)\n",
    "        assert (\n",
    "            np.sum(query_vecs) != 0\n",
    "        ), \"오류가 발생했습니다. 이 오류는 보통 query에 vectorizer의 vocab에 없는 단어만 존재하는 경우 발생합니다.\"\n",
    "\n",
    "        q_embs = query_vecs.toarray().astype(np.float32)\n",
    "        D, I = self.indexer.search(q_embs, k)\n",
    "\n",
    "        return D.tolist(), I.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276b2697",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.retriever_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce2e13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = SparseRetrieval(\n",
    "    retrieval_path = args.retriever_path,\n",
    "    retrieval_type=args.vectorizer_type,\n",
    "    retrieval_parameters=args.vectorizer_parameters,\n",
    "    tokenize_fn=tokenizer.tokenize if args.tokenizer_type == \"AutoTokenizer\" else tokenizer.morphs,\n",
    "    output_path=output_path,\n",
    "    data_path=args.data_path,\n",
    "    context_path=args.context_path,\n",
    "    num_clusters = args.num_clusters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6deb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"대통령을 포함한 미국의 행정부 견제권을 갖는 국가 기관은?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf7702",
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer(\"single query by exhaustive search\"):\n",
    "    scores, indices = retriever.retrieve(query, topk=args.top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ed7e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "len(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c05b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_or_dataset=full_ds\n",
    "print(query_or_dataset['question'][0])\n",
    "print(query_or_dataset['answers'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c0e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_scores, doc_indices = retriever.get_relevant_doc_bulk(query_or_dataset['question'][:3], k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c714517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(doc_scores), len(doc_scores[0]))\n",
    "print(len(doc_indices), len(doc_indices[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb91690",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, example in enumerate(tqdm(query_or_dataset, desc=\"Sparse retrieval: \")):\n",
    "    pprint(example)\n",
    "    print('====')\n",
    "    print(f'\"question\": {example[\"question\"]}')\n",
    "    print(f'\"id\": {example[\"id\"]}')\n",
    "    print(f'\"context_id\": {doc_indices[idx]}')\n",
    "    print(f'\"answers\": {example[\"answers\"]}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c6f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = \" \".join([retriever.contexts[pid] for pid in doc_indices[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a7d21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58357d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
