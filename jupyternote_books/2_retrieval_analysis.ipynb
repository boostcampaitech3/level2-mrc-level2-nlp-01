{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ae7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import utils\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3041c741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--use_faiss'], dest='use_faiss', nargs=None, const=None, default=False, type=<class 'bool'>, choices=None, help='', metavar=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MYDICT = {'key': 'value'}\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "parser.add_argument(\n",
    "    \"--config_retrieval\",\n",
    "    default=\"./config/retrieval_config.json\",\n",
    "    metavar=\"./config/retrieval_config.json\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--vectorizer_type\",\n",
    "    default=\"TfidfVectorizer\",\n",
    "    metavar=\"TfidfVectorizer\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument('--vectorizer_parameters',\n",
    "                    type=json.loads, default=MYDICT)\n",
    "parser.add_argument(\n",
    "    \"--tokenizer_type\",\n",
    "    default=\"AutoTokenizer\",\n",
    "    metavar=\"AutoTokenizer\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--dataset_name\", default = \"./data/train_dataset\",\n",
    "    metavar=\"./data/train_dataset\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--model_name_or_path\",\n",
    "    default =\"bert-base-multilingual-cased\",\n",
    "    metavar=\"bert-base-multilingual-cased\",\n",
    "    type=str,\n",
    "    help=\"\",\n",
    ")\n",
    "parser.add_argument(\"--data_path\",default = \"./data\",\n",
    "                    metavar=\"./data\", type=str, help=\"\")\n",
    "parser.add_argument(\n",
    "    \"--context_path\",\n",
    "    default = \"wikipedia_documents\",\n",
    "    metavar=\"wikipedia_documents\", type=str, help=\"\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--output_path\",\n",
    "    default=\"./retriever_result\",\n",
    "    metavar=\"./retriever_result\", type=str, help=\"\"\n",
    ")\n",
    "\n",
    "parser.add_argument(\"--use_faiss\", default=False, metavar=False, type=bool, help=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6bbefa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './config/retrieval_config.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n\u001b[0;32m----> 2\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig_retrieval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m parser\u001b[38;5;241m.\u001b[39mset_defaults(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig)\n\u001b[1;32m      4\u001b[0m args \u001b[38;5;241m=\u001b[39m parser\u001b[38;5;241m.\u001b[39mparse_args([])\n",
      "File \u001b[0;32m~/input/code/utils/utils_configs.py:7\u001b[0m, in \u001b[0;36mread_json\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_json\u001b[39m(file):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m      8\u001b[0m         data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './config/retrieval_config.json'"
     ]
    }
   ],
   "source": [
    "args = parser.parse_args([])\n",
    "config = utils.read_json(args.config_retrieval)\n",
    "parser.set_defaults(**config)\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efa1e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e1bbc2",
   "metadata": {},
   "source": [
    "## 1. arrow dataest analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd38fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, concatenate_datasets, load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944771c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sparse\n",
    "args.dataset_name = '../data/train_dataset'\n",
    "org_dataset = load_from_disk(args.dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705bcc8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ds = concatenate_datasets(\n",
    "    [\n",
    "        org_dataset[\"train\"].flatten_indices(),\n",
    "        org_dataset[\"validation\"].flatten_indices(),\n",
    "    ]\n",
    ")  # train dev 를 합친 4192 개 질문에 대해 모두 테스트\n",
    "print(\"*\" * 40, \"query dataset\", \"*\" * 40)\n",
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75090c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da9e7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(full_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45429ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(full_ds[\"question\"]))\n",
    "print(len(full_ds[\"context\"]))\n",
    "print(len(full_ds[\"answers\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (q, c, a) in enumerate(zip(full_ds[\"question\"], full_ds[\"context\"], full_ds[\"answers\"])):\n",
    "    print(f'question : {q}')\n",
    "    print(f'contenxt : {c}')\n",
    "    print(f'answers : {a}')\n",
    "    if index == 5:\n",
    "        break\n",
    "#     if len(a['text']) > 1:\n",
    "#         print(f'question : {q}')\n",
    "#         print(f'contenxt : {c}')\n",
    "#         print(f'answers : {a}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d269cd",
   "metadata": {},
   "source": [
    "## 2. setting tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98109a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import konlpy.tag\n",
    "from transformers import AutoTokenizer\n",
    "from importlib import import_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb69fb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.tokenizer_type = \"Mecab\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de4666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(import_module(\"transformers\"), args.tokenizer_type):\n",
    "    tokenizer_type = getattr(import_module(\"transformers\"), args.tokenizer_type)\n",
    "    tokenizer = tokenizer_type.from_pretrained(args.model_name_or_path, use_fast=False,)\n",
    "    print(f'{args.tokenizer_type}')\n",
    "elif hasattr(import_module(\"konlpy.tag\"), args.tokenizer_type):\n",
    "    tokenizer = getattr(import_module(\"konlpy.tag\"), args.tokenizer_type)()\n",
    "    print(f'{args.tokenizer_type}')\n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type - {args.tokenizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3af3615",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65bdb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.tokenize('일등이 아니어도 괜찮아')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2 = getattr(import_module(\"konlpy.tag\"), \"Mecab\")()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ab4111",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer2.morphs('일등이 아니어도 괜찮아')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcae106",
   "metadata": {},
   "source": [
    "## 3. Setting Sparse Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b946a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(args.vectorizer_type)\n",
    "print(args.vectorizer_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = SparseRetrieval(\n",
    "#     retrieval_type = args.retrieval_type,\n",
    "#     retrieval_parameters = args.retrieval_parameters,\n",
    "#     tokenize_fn=tokenizer.tokenize if args.tokenizer_type == \"AutoTokenizer\" else tokenizer.morphs,\n",
    "#     data_path=args.data_path,\n",
    "#     context_path=args.context_path,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55573cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, NoReturn, Optional, Tuple, Union\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43774b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/wikipedia_documents.json\"\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4cd978",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df52325",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = [v[\"text\"] for v in wiki.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55df0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 순서대로 중복 제거\n",
    "# ('key1', 'key2', 'key3', 'key1', 'key4', 'key2') -> ['key1', 'key2', 'key3', 'key4']\n",
    "contexts = list(\n",
    "    dict.fromkeys([v[\"text\"] for v in wiki.values()])\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349f24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(contexts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1738b",
   "metadata": {},
   "source": [
    "## 4. setting retrieval_type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e557901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import retriever\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0283ec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = retriever.BM25()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac76ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'args.vectorizer_type : {args.vectorizer_type}\\n')\n",
    "print(f'args.vectorizer_parameters : {args.vectorizer_parameters}\\n')\n",
    "print(f'tokenizer : {tokenizer}')\n",
    "print(f'tokenizer.tokenize : {tokenizer.tokenize}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b433ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform by vectorizer\n",
    "# tfidfv = TfidfVectorizer(\n",
    "#     tokenizer=tokenize_fn, **args.retrieval_parameters\n",
    "# )\n",
    "# print(tfidfv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36759359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c333e2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(hasattr(import_module(\"retriever\"), \"BM25\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89c1581",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_fn_1 = tokenizer.tokenize\n",
    "print(tokenize_fn_1, '\\n')\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "tokenizer2 = Mecab()\n",
    "tokenize_fn_2 =tokenizer2.morphs\n",
    "print(tokenize_fn_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9974bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vectorizer_type = \"TfidfVectorizer\"\n",
    "tokenize_fn = tokenize_fn_1\n",
    "\n",
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaef348",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vectorizer_type = \"TfidfVectorizer\"\n",
    "tokenize_fn = tokenize_fn_2\n",
    "\n",
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    print(f'tokenize_fn : {tokenize_fn}')\n",
    "    print(f'vectorizer_type : {vectorizer_type}')\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer.vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f02427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vectorizer_type = \"BM25\"\n",
    "tokenize_fn = tokenize_fn_2\n",
    "\n",
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    print(f'tokenize_fn : {tokenize_fn}')\n",
    "    print(f'vectorizer_type : {vectorizer_type}')\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer.vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfd3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vectorizer_type = \"BM25\"\n",
    "tokenize_fn = tokenize_fn_1\n",
    "\n",
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    print(f'tokenize_fn : {tokenize_fn}')\n",
    "    print(f'vectorizer_type : {vectorizer_type}')\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer.vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b05693",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.vectorizer_type = \"BM25\"\n",
    "tokenize_fn = tokenize_fn_2\n",
    "\n",
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    print(f'tokenize_fn : {tokenize_fn}')\n",
    "    print(f'vectorizer_type : {vectorizer_type}')\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer.vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if hasattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"sklearn.feature_extraction.text\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenizer=tokenize_fn, **args.vectorizer_parameters)\n",
    "    print(f'{vectorizer}')\n",
    "    \n",
    "elif hasattr(import_module(\"retriever\"), args.vectorizer_type):\n",
    "    vectorizer_type = getattr(import_module(\"retriever\"), args.vectorizer_type)\n",
    "    vectorizer = vectorizer_type(tokenize_fn, args.vectorizer_parameters)\n",
    "    print(f'{vectorizer.vectorizer}')\n",
    "    \n",
    "else:\n",
    "    raise Exception(f\"Use correct tokenizer type : Current tokenizer : {args.vectorizer_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4eaf6b1",
   "metadata": {},
   "source": [
    "### 4. setting retrieval result directory with incrementing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adc425c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pprint(vars(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3e3bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(args.output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02c7e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tokenizer_type == \"AutoTokenizer\":\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.model_name_or_path}_{args.context_path}'\n",
    "else:\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.tokenizer_type}_{args.context_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c936b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34795e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i = 1\n",
    "# while True:\n",
    "#     path = f'{output_path}_{i}/'\n",
    "#     print(path)\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "#         break\n",
    "#     i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccdf108",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af937cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "output_path = utils.increment_directory(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa303d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7049e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.tokenizer_type == \"AutoTokenizer\":\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.model_name_or_path}_{args.context_path}'\n",
    "else:\n",
    "    output_path = args.output_path + f'/{args.vectorizer_type}_{args.tokenizer_type}_{args.context_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a16940",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
